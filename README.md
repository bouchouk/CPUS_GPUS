# CPUS_GPUS
## Project Overview
In this project, we aimed to train variance models using variance datasets while varying the number of CPUs and GPUs. The goal was to analyze the performance of these models across different configurations of computational resources.

## Implementation
In this implementation, we focused on parallelizing our models specifically on the CPUs. The model training process was adjusted to scale across multiple CPU cores to see the performance improvements on the output_cpu.csv file.
