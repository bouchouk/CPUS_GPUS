6


----------------------------------------------------------------------------
Training LightGBM with config/adult.yml in env gbdt

Namespace(config='config/adult.yml', model_name='LightGBM', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[2]	eval's auc: 0.896125
{'Log Loss - mean': 0.5319043205643598, 'Log Loss - std': 0.0, 'AUC - mean': 0.8961249358010481, 'AUC - std': 0.0, 'Accuracy - mean': 0.7590971902349147, 'Accuracy - std': 0.0, 'F1 score - mean': 0.7590971902349147, 'F1 score - std': 0.0}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.899139
{'Log Loss - mean': 0.5268965814725322, 'Log Loss - std': 0.005007739091827668, 'AUC - mean': 0.8976320541543217, 'AUC - std': 0.001507118353273551, 'Accuracy - mean': 0.759155474724337, 'Accuracy - std': 5.828448942224451e-05, 'F1 score - mean': 0.759155474724337, 'F1 score - std': 5.828448942224451e-05}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.904335
{'Log Loss - mean': 0.5249603723575763, 'Log Loss - std': 0.004920986894403297, 'AUC - mean': 0.8998664531837103, 'AUC - std': 0.003391068936522489, 'Accuracy - mean': 0.7591749028874778, 'Accuracy - std': 5.495114361128626e-05, 'F1 score - mean': 0.7591749028874778, 'F1 score - std': 5.495114361128626e-05}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.898927
{'Log Loss - mean': 0.5241320437416803, 'Log Loss - std': 0.004496717569394689, 'AUC - mean': 0.8996314706317066, 'AUC - std': 0.002964820702576192, 'Accuracy - mean': 0.7591846169690482, 'Accuracy - std': 5.047584848626914e-05, 'F1 score - mean': 0.7591846169690482, 'F1 score - std': 5.047584848626914e-05}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.892169
{'Log Loss - mean': 0.5238280361918439, 'Log Loss - std': 0.004067684535525391, 'AUC - mean': 0.8981389020890453, 'AUC - std': 0.003992890288613877, 'Accuracy - mean': 0.7591904454179904, 'Accuracy - std': 4.662759153779561e-05, 'F1 score - mean': 0.7591904454179904, 'F1 score - std': 4.662759153779561e-05}
Results: {'Log Loss - mean': 0.5238280361918439, 'Log Loss - std': 0.004067684535525391, 'AUC - mean': 0.8981389020890453, 'AUC - std': 0.003992890288613877, 'Accuracy - mean': 0.7591904454179904, 'Accuracy - std': 4.662759153779561e-05, 'F1 score - mean': 0.7591904454179904, 'F1 score - std': 4.662759153779561e-05}
Train time: 0.20568394200000006
Inference time: 0.027312217600000112
Finished cross validation
{'Log Loss - mean': 0.5238280361918439, 'Log Loss - std': 0.004067684535525391, 'AUC - mean': 0.8981389020890453, 'AUC - std': 0.003992890288613877, 'Accuracy - mean': 0.7591904454179904, 'Accuracy - std': 4.662759153779561e-05, 'F1 score - mean': 0.7591904454179904, 'F1 score - std': 4.662759153779561e-05}
(0.20568394200000006, 0.027312217600000112)


----------------------------------------------------------------------------
Training LightGBM with config/california_housing.yml in env gbdt

Namespace(config='config/california_housing.yml', model_name='LightGBM', dataset='CaliforniaHousing', objective='regression', use_gpu=True, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='minimize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=False, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=8, num_classes=1, cat_idx=None, cat_dims=None)
Train model with given hyperparameters
Loading dataset CaliforniaHousing...
Dataset loaded!
(20640, 8)
Scaling the data...
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's l2: 1.03283
{'MSE - mean': 1.032832166061334, 'MSE - std': 0.0, 'R2 - mean': 0.2384901286155472, 'R2 - std': 0.0}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's l2: 0.993036
{'MSE - mean': 1.0129342131349102, 'MSE - std': 0.019897952926423768, 'R2 - mean': 0.23399669287604685, 'R2 - std': 0.00449343573950034}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's l2: 1.00232
{'MSE - mean': 1.0093972283598147, 'MSE - std': 0.016999202227554664, 'R2 - mean': 0.2354182674494183, 'R2 - std': 0.00418358598557008}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's l2: 1.07336
{'MSE - mean': 1.0253867826661365, 'MSE - std': 0.031364425676623175, 'R2 - mean': 0.233803368993995, 'R2 - std': 0.0045771699594814395}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's l2: 0.996951
{'MSE - mean': 1.0196996308094597, 'MSE - std': 0.03027138159026019, 'R2 - mean': 0.2339835473510014, 'R2 - std': 0.004109774304231025}
Results: {'MSE - mean': 1.0196996308094597, 'MSE - std': 0.03027138159026019, 'R2 - mean': 0.2339835473510014, 'R2 - std': 0.004109774304231025}
Train time: 0.06963528559999994
Inference time: 0.00569746379999998
Finished cross validation
{'MSE - mean': 1.0196996308094597, 'MSE - std': 0.03027138159026019, 'R2 - mean': 0.2339835473510014, 'R2 - std': 0.004109774304231025}
(0.06963528559999994, 0.00569746379999998)


----------------------------------------------------------------------------
Training LightGBM with config/covertype.yml in env gbdt

Namespace(config='config/covertype.yml', model_name='LightGBM', dataset='Covertype', objective='classification', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='minimize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=54, num_classes=7, cat_idx=None, cat_dims=None)
Train model with given hyperparameters
Loading dataset Covertype...
Dataset loaded!
(581012, 54)
Having 7 classes as target.
Scaling the data...
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's multi_logloss: 1.01824
{'Log Loss - mean': 1.0182401546256974, 'Log Loss - std': 0.0, 'AUC - mean': 0.9447506542614053, 'AUC - std': 0.0, 'Accuracy - mean': 0.5070781305129816, 'Accuracy - std': 0.0, 'F1 score - mean': 0.36123155617821806, 'F1 score - std': 0.0}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's multi_logloss: 1.01784
{'Log Loss - mean': 1.0180393364183726, 'Log Loss - std': 0.00020081820732464895, 'AUC - mean': 0.9450194412795474, 'AUC - std': 0.000268787018142147, 'Accuracy - mean': 0.5120607901689285, 'Accuracy - std': 0.004982659655946908, 'F1 score - mean': 0.37113429412243165, 'F1 score - std': 0.009902737944213591}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's multi_logloss: 1.0191
{'Log Loss - mean': 1.0183920691691029, 'Log Loss - std': 0.0005250962659081212, 'AUC - mean': 0.9458078725214003, 'AUC - std': 0.0011364030771138934, 'Accuracy - mean': 0.5082241151283101, 'Accuracy - std': 0.00678169710072212, 'F1 score - mean': 0.3631367853025777, 'F1 score - std': 0.013903109023109228}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's multi_logloss: 1.01909
{'Log Loss - mean': 1.0185661634903875, 'Log Loss - std': 0.0005456382175615693, 'AUC - mean': 0.9459310816832966, 'AUC - std': 0.0010070255498154062, 'Accuracy - mean': 0.5075127275744387, 'Accuracy - std': 0.006000981455550982, 'F1 score - mean': 0.36175328166220977, 'F1 score - std': 0.012276586551239723}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's multi_logloss: 1.01887
{'Log Loss - mean': 1.0186260459159355, 'Log Loss - std': 0.0005025141501778938, 'AUC - mean': 0.945848825001659, 'AUC - std': 0.0009156118244545397, 'Accuracy - mean': 0.5106056279210679, 'Accuracy - std': 0.008189844501613685, 'F1 score - mean': 0.36805504954836776, 'F1 score - std': 0.01671588393846138}
Results: {'Log Loss - mean': 1.0186260459159355, 'Log Loss - std': 0.0005025141501778938, 'AUC - mean': 0.945848825001659, 'AUC - std': 0.0009156118244545397, 'Accuracy - mean': 0.5106056279210679, 'Accuracy - std': 0.008189844501613685, 'F1 score - mean': 0.36805504954836776, 'F1 score - std': 0.01671588393846138}
Train time: 4.0779229366
Inference time: 0.6018051276000005
Finished cross validation
{'Log Loss - mean': 1.0186260459159355, 'Log Loss - std': 0.0005025141501778938, 'AUC - mean': 0.945848825001659, 'AUC - std': 0.0009156118244545397, 'Accuracy - mean': 0.5106056279210679, 'Accuracy - std': 0.008189844501613685, 'F1 score - mean': 0.36805504954836776, 'F1 score - std': 0.01671588393846138}
(4.0779229366, 0.6018051276000005)
